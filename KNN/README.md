## KNN

K Nearest Neighbor, KNN, 又稱 K-近鄰演算法
屬於監督是學習(supervised learning)其中一種算法
顧名思義就是k個最接近你的**鄰居**
用 Sklearn 建立KNN模型。
其中分類的表準，由鄰居多數表決決定，KNN可用在**分類**或是**回歸**
流程如下，
1. 決定K值
2. 計算每個鄰居與自己之間的距離
3. 找出跟自己最近的鄰居，哪個鄰居數量多就加入哪一組。<br>


K 的大小會影響最終分類的結果

####KNN回歸器
回歸模型預測方法為最近的K個**數值平均**
假設`k = 2, input = x, output = y`選擇最近的兩個 x 平均後為 y。
判斷哪些是鄰居，首先要量化相似度，也就是最常見的歐幾里得距離(Euclidean distance)公式如下：
`sqrt(sum((x - y) ^ 2))`
KNN 的缺點是對局部的資料非常敏感，k 值扮演重大的腳色。

由於鳶尾花朵大致分為三種，應用分類而非預測，
範例程式 **KNNRegressor.py** 中可能不適合拿鳶尾花朵資料集做訓練，
否則將與預測圖類似。


---
KNN VS. K-means
兩者相似的點為皆須調整 K 值，但切勿混淆。
KNN 的 K 值為調整鄰居數量，作為輸出預測的依據。
K-means 的 K 值為調整群體的數量，中心圓的數量。
